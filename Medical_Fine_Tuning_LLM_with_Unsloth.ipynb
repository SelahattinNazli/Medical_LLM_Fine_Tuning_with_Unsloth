{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Environment Setup & Dependencies Installation**\n"
      ],
      "metadata": {
        "id": "LRLiQZ3ZpGFh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzHrIc1hbLSg"
      },
      "outputs": [],
      "source": [
        "# Environment detection - simple and clear\n",
        "import os\n",
        "IN_COLAB = 'COLAB_GPU' in os.environ\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"Installing dependencies for Colab environment...\")\n",
        "    !pip install -q unsloth \"xformers<0.0.30\" trl peft bitsandbytes\n",
        "else:\n",
        "    print(\"Local environment detected\")\n",
        "    !pip install -q unsloth\n",
        "\n",
        "# Common libraries\n",
        "!pip install -q datasets transformers accelerate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Environment Setup**\n",
        "\n",
        "This section automatically detects whether the code is running in a Google Colab environment.  \n",
        "- If running in **Colab**, it installs all required libraries for fine-tuning (`unsloth`, `xformers`, `trl`, `peft`, `bitsandbytes`).  \n",
        "\n",
        "**Installed libraries overview (Colab):**\n",
        "- **unsloth:** Optimized fine-tuning framework for large language models (fast, memory-efficient, and 4-bit ready).  \n",
        "- **xformers:** Metaâ€™s optimized attention library that speeds up Transformer operations on GPUs.  \n",
        "- **trl:** Hugging Faceâ€™s library for supervised and reinforcement learning-based fine-tuning.  \n",
        "- **peft:** Parameter-Efficient Fine-Tuning toolkit (LoRA, prefix-tuning, etc.).  \n",
        "- **bitsandbytes:** Enables 8-bit and 4-bit quantization for reduced GPU memory usage during training.  \n",
        "\n",
        "- If running **locally**, it only installs the core `unsloth` package.  \n",
        "\n",
        "**Common libraries installed in both cases:**\n",
        "- **datasets:** Easy access to datasets from Hugging Face for training and evaluation.  \n",
        "- **transformers:** Provides tokenizers and pretrained model classes for loading and training LLMs.  \n",
        "- **accelerate:** Handles device placement, mixed precision, and distributed training efficiently.\n",
        "\n"
      ],
      "metadata": {
        "id": "_feUsW6GpdPn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Model Loading & Configuration**\n"
      ],
      "metadata": {
        "id": "MXgdrCKJrN1C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3bcagj_bs6H"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# Model configuration\n",
        "max_seq_length = 1024  # Reduced for T4 memory efficiency\n",
        "dtype = None  # Auto detection: Float16 for T4, Bfloat16 for Ampere+\n",
        "load_in_4bit = True  # Essential for T4 GPU memory\n",
        "\n",
        "# Load model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\",  # Use if accessing gated models\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Loading & Configuration**\n",
        "\n",
        "In this section, we load the **Llama-3.2-3B-Instruct** model using the `Unsloth` library, which provides optimized fine-tuning and inference performance for low-memory environments (like Google Colab T4 GPUs).\n",
        "\n",
        "**What this code does:**\n",
        "- **Imports:** Loads `FastLanguageModel` from `unsloth` and `torch` for tensor operations.\n",
        "- **Model parameters:**\n",
        "  - `max_seq_length = 1024` â†’ Limits the modelâ€™s context window to 1024 tokens for better memory efficiency on T4 GPUs.  \n",
        "    (It can be increased to 2048, but that would roughly double the memory usage and may cause OOM errors.)\n",
        "  - `dtype = None` â†’ Lets Unsloth automatically choose the most suitable precision (Float16 for T4, BFloat16 for newer GPUs).\n",
        "  - `load_in_4bit = True` â†’ Loads the model in 4-bit quantized format, significantly reducing GPU memory usage.\n",
        "\n",
        "**Why this model?**\n",
        "\n",
        "The **Llama-3.2-3B-Instruct** model offers a strong balance between:\n",
        "- Quality (instruction-tuned for conversation and reasoning)\n",
        "- Speed (smaller than 7B/13B models)\n",
        "- Efficiency (fits within Colabâ€™s free-tier GPU memory limits)\n",
        "\n",
        "Overall, this setup ensures that fine-tuning can run smoothly and efficiently on limited GPU resources while still maintaining high model quality.\n",
        "\n"
      ],
      "metadata": {
        "id": "1sxqmRZ2r7EW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. LoRA Configuration (Parameter-Efficient Fine-Tuning)**\n"
      ],
      "metadata": {
        "id": "Il_2L2vPszoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 64,  # LoRA rank - balanced for T4\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha = 128,  # 2x rank for better learning\n",
        "    lora_dropout = 0,  # Optimized: 0 for better performance\n",
        "    bias = \"none\",  # Optimized: no bias training\n",
        "    use_gradient_checkpointing = \"unsloth\",  # Memory efficient\n",
        "    random_state = 3407,  # Reproducibility seed\n",
        "    use_rslora = True,  # Rank-Stabilized LoRA for stability\n",
        "    loftq_config = None,\n",
        ")"
      ],
      "metadata": {
        "id": "W5WC-l0PhAul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LoRA Configuration (Parameter-Efficient Fine-Tuning)**\n",
        "\n",
        "In this section, the pre-loaded Llama model is converted into a **LoRA-enabled fine-tuning model** using the `Unsloth` PEFT integration.  \n",
        "Instead of training *all* model weights, LoRA updates only a few low-rank adapter layers â€” making training much faster and memory-efficient.\n",
        "\n",
        "**What this code does:**\n",
        "- Uses `FastLanguageModel.get_peft_model()` to wrap the model with LoRA layers.\n",
        "- Targets specific transformer layers (`q_proj`, `k_proj`, `v_proj`, `o_proj`, etc.) that are most influential in learning context and attention patterns.\n",
        "\n",
        "**Parameter explanations:**\n",
        "- **`r = 64`** â†’ LoRA rank. Controls how much capacity the adapters have to learn new information.  \n",
        "  A value of 64 provides a balanced trade-off between learning power and memory efficiency on T4 GPUs.\n",
        "- **`lora_alpha = 128`** â†’ Scaling factor, typically set to 2Ã— the rank for stable training.\n",
        "- **`lora_dropout = 0`** â†’ No dropout applied; this yields better performance for smaller datasets.\n",
        "- **`use_gradient_checkpointing = \"unsloth\"`** â†’ Saves GPU memory by recomputing intermediate values on demand.\n",
        "- **`use_rslora = True`** â†’ Enables *Rank-Stabilized LoRA* for more stable convergence.\n",
        "- **`bias = \"none\"`** â†’ Bias parameters are not trained to keep optimization simple and efficient.\n",
        "\n",
        "**Why this setup?**\n",
        "\n",
        "This configuration is specifically tuned for **Google Colab T4 GPUs**:\n",
        "- 4-bit quantization keeps the model lightweight.\n",
        "- LoRA reduces the number of trainable parameters by ~99%.\n",
        "- The chosen rank and modules ensure a strong learning signal without exceeding memory limits.\n",
        "\n",
        "Overall, this makes fine-tuning feasible on limited hardware while still achieving high-quality results.\n"
      ],
      "metadata": {
        "id": "1_3grY8ctP5u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Dataset Loading & Preprocessing**\n"
      ],
      "metadata": {
        "id": "kaJdGFq8vw3M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Loading & Preprocessing**\n",
        "\n",
        "In this section, we prepare the dataset for fine-tuning by:\n",
        "\n",
        "- Loading the ```Python Code Instructions 18K Alpaca``` dataset from **Hugging Face**.\n",
        "\n",
        "- Exploring its structure, which includes instruction, input, and output fields designed for instruction-based code generation tasks.\n",
        "\n",
        "- Inspecting sample entries to understand how programming-related prompts and solutions are formatted.\n"
      ],
      "metadata": {
        "id": "I3rjLQnEv58e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load medical dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"medalpaca/medical_meadow_medical_flashcards\", split=\"train\")\n",
        "\n",
        "# Dataset info\n",
        "print(f\"Dataset size: {len(dataset)}\")\n",
        "print(f\"Columns: {dataset.column_names}\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# Show first 3 examples\n",
        "for i in range(3):\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(dataset[i])\n"
      ],
      "metadata": {
        "id": "jYeALThc_l_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.1. Split Dataset**"
      ],
      "metadata": {
        "id": "y2XAT3hET7ph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset - take 10% for efficient training\n",
        "dataset = dataset.train_test_split(train_size=0.1)[\"train\"]\n",
        "\n",
        "print(f\"Training samples: {len(dataset)}\")\n",
        "print(f\"\\nFirst example after split:\")\n",
        "print(dataset[0])"
      ],
      "metadata": {
        "id": "4J_rA9rFAIs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.2. Alpaca Prompt Format**"
      ],
      "metadata": {
        "id": "JdEgyCziUDbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Medical Alpaca prompt\n",
        "alpaca_prompt = \"\"\"You are an expert doctor in the field of medicine.\n",
        "Answer the following medical question professionally and accurately.\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Medical Answer:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def formatting_medical(examples):\n",
        "    inputs = examples[\"input\"]\n",
        "    outputs = examples[\"output\"]\n",
        "    texts = []\n",
        "\n",
        "    for inp, out in zip(inputs, outputs):\n",
        "        text = alpaca_prompt.format(inp, out) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "\n",
        "    return {\"text\": texts}"
      ],
      "metadata": {
        "id": "H9I12JCLvVpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2ï¸âƒ£ Define formatting function\n",
        "dataset = dataset.map(formatting_medical, batched=True)"
      ],
      "metadata": {
        "id": "UOZMIYmOvR5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "avm1IQsFwR3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check first example\n",
        "print(dataset[0]['text'][:500])"
      ],
      "metadata": {
        "id": "Oz5yPYbDle5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.1. Mount Google Drive**"
      ],
      "metadata": {
        "id": "Oqs_d5j_zBTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive for saving model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ukibR5XoifHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Saving Setup (Google Drive)**\n",
        "\n",
        "Google Drive is mounted to store fine-tuned models and checkpoints.  \n",
        "This ensures training outputs are not lost when the Colab runtime resets.\n"
      ],
      "metadata": {
        "id": "qWZT7TwNzGTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Training Configuration & Initialization**"
      ],
      "metadata": {
        "id": "swDJDgkSzLqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training configuration - MEDICAL MODEL\n",
        "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
        "\n",
        "trainer = UnslothTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "\n",
        "    args = UnslothTrainingArguments(\n",
        "        # Optimize for speed and quality\n",
        "        per_device_train_batch_size = 6,\n",
        "        gradient_accumulation_steps = 2,\n",
        "\n",
        "        # Full epoch training\n",
        "        num_train_epochs = 1,\n",
        "        warmup_ratio = 0.05,\n",
        "\n",
        "        # Learning rates\n",
        "        learning_rate = 5e-5,\n",
        "        embedding_learning_rate = 1e-6,\n",
        "\n",
        "        # Optimization\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"cosine\",\n",
        "        seed = 3407,\n",
        "\n",
        "        # Logging and saving\n",
        "        logging_steps = 30,\n",
        "        save_strategy = \"steps\",\n",
        "        save_steps = 150,\n",
        "        save_total_limit = 2,\n",
        "\n",
        "        # Output\n",
        "        output_dir = \"/content/drive/MyDrive/unsloth_medical_assistant\",\n",
        "        report_to = \"none\",\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "8Q5Prk1u85yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5.1. Show Current Memory Stats**"
      ],
      "metadata": {
        "id": "EBeadwhPi2Hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "metadata": {
        "id": "DW7VQDtdzfXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5.2. Start Training**"
      ],
      "metadata": {
        "id": "K-nXUIJG0IOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start Training\n",
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "0Qyw8NfyjDx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5.3. Save Trained Model**"
      ],
      "metadata": {
        "id": "aK3k-wSba_9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/unsloth_medical_final\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/unsloth_medical_final\")"
      ],
      "metadata": {
        "id": "W7BTHutM2xuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Inference / Text Generation**\n"
      ],
      "metadata": {
        "id": "fN9gSPKJclmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "alpaca_prompt = \"\"\"You are an expert doctor in the field of medicine.\n",
        "Answer the following medical question professionally and accurately.\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Medical Answer:\n",
        "\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# âœ… 1. Single sanity test (optional)\n",
        "# ============================================================\n",
        "test_prompt = alpaca_prompt.format(\"What are the common symptoms of diabetes?\")\n",
        "\n",
        "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=200,\n",
        "    temperature=0.5,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.2,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "print(\"\\nðŸ©º Single test output:\\n\")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# âœ… 2. Multi-question test set (Alpaca format)\n",
        "# ============================================================\n",
        "test_questions = [\n",
        "    \"What are the common symptoms of diabetes?\",\n",
        "    \"What is hypertension?\",\n",
        "    \"What causes migraine headaches?\",\n",
        "    \"Explain the function of insulin in the human body.\",\n",
        "    \"What are the side effects of antibiotics?\",\n",
        "    \"How does dehydration affect kidney function?\",\n",
        "    \"What are the risk factors for heart disease?\",\n",
        "    \"How does the immune system respond to viral infections?\",\n",
        "    \"Explain the difference between Type 1 and Type 2 diabetes.\",\n",
        "    \"What are the benefits of regular exercise for cardiovascular health?\",\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "for i, question in enumerate(test_questions, start=1):\n",
        "    prompt = alpaca_prompt.format(question)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=200,\n",
        "        temperature=0.5,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.2,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    results.append({\"question\": question, \"answer\": answer})\n",
        "    print(f\"\\n[{i}] Question: {question}\\nAnswer:\\n{answer[:500]}\\n{'-'*80}\")\n"
      ],
      "metadata": {
        "id": "7tYhsLZ9-tqh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}